{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Basket Text Analysis of One Health and Antimicrobial Research Papers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### What is Market Basket Analysis?\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Too long; Didn't Read: We're looking for items or words that appear commonly together.\n",
    "\n",
    "Consider a grocery store. Over the course of a month, **1000s** of people while have stumbled through our example grocery store. Some customers, like my parents, may pile up shopping carts to the cieling with microwave dinners. Customers a little more like me, or the average college student, might walk out with fewer than a dozen items. \n",
    "\n",
    "Consider how many different items exist in any given grocery store. Ghetto Kroger probably has several hundred items while Gucci Kroger has maybe more than a thousand? \n",
    "\n",
    "Out of all the possible things we could buy in a grocery store, customers end up only buying a select few. On top of that, we could reasonably guess that most purchases are biased towards a few items. I buy a half-gallon of milk, a loaf of bread, an a box of Lucky Charms when I go to the grocery store.\n",
    "\n",
    "Finding the most popular item is trivial. We can just keep a running count for each unique item and use a bar chart for visualization. Finding the most popular combination of items out of **every possible combination** is not so trivial. \n",
    "\n",
    "Think about the dataframe that would hold all of this. We'd have a column for every item and a row for every receipt. This dataframe could easily get to hundres of thousands of observations for several thousand variables in any decent suburb. Additionally, this matrix also tends to be sparse, meaning that almost entries are '0' or empty.\n",
    "\n",
    "#### The Solution\n",
    "\n",
    "Enter Association Rules or Market Basket Analysis. This tool uses a few simple steps to find probabilites that: given 'item a' ('on the left', 'leftside') is purchased, 'item b' ('on the right', 'rightside') is also purchased.\n",
    "\n",
    "Mathematically, let $X$, $Y$ be itemsets and $T$ be the set of all transactions taken place. Then an association rule looks simply like, \n",
    "$$ X \\Rightarrow Y $$\n",
    "\n",
    "Three terms we'll need to define:\n",
    "\n",
    "Too long; Didn't Read: Use a couple rules to find probably associated items in a dataset.\n",
    "\n",
    "1. Support - How many times the leftside appears in the dataset\n",
    "$$ supp(X) =  \\frac{|t \\in T; X \\in t|}{|T|}$$\n",
    "\n",
    "2. Confidence - The proportion of all transactions that contain both the left and right side. Or, a little more correctly, given a transaction contains the left side ($X$) the transaction also contains the right side ($Y$)\n",
    "$$ conf (X \\Rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)} $$\n",
    "\n",
    "My Latex is not quite pro, yet. My $f$ in the above looks a little whack, and I can't figure it out.\n",
    "\n",
    "3. Lift - The ratio of a given rule compared to what would be expected if th left and right sides truly appeared independent of one another.\n",
    "$$ lift(X \\Rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X) \\times supp(Y)} $$\n",
    "\n",
    "These are 'measures of interestingness'. There are others, but, in my mind, these are 'the big three'.\n",
    "\n",
    "Finding Association Rules is a two step process:\n",
    "\n",
    "1. Set some minimum support threshold to find frequent itemsets in the transaction database. Itemsets with less than this support are thrown out.\n",
    "2. Set some minimum confidence threshold to these supported itemsets in order to form rules. Rules with less than this confidence are thrown out.\n",
    "\n",
    "Once we have a few itemsets, the second step seems trivial to apply. The first step, at a glance, may seem less trivial. The first step seems to involve searching through every possible combination of itemsets to meet our given support threshold. Well, in short, the support rule has a property ('downward-closure') which, for us, boils down to 'unsupported itemsets can't be contained in larger supported itemsets'. This property is what algorithms like [Apriori](http://rakesh.agrawal-family.com/papers/vldb94apriori.pdf) take advantage of to efficiently search these super large, sparse transaction databases.\n",
    "\n",
    "Lift is not directly used in the process mentioned above. Instead, lift really tells us what a given rule ($ X \\Rightarrow Y$) *means*. If the reported lift is:\n",
    "\n",
    "1. Equal to one - There is no relation between $X$ and $Y$. People buying cereal may or may not buy bread.\n",
    "2. Greater than one - $X$ and $Y$ seem to be dependent on another. When people buy milk, they usually also buy cereal.\n",
    "3. Less than one - Either itemset in this rule has a negative impact on the presence of the other itemset. People buying cereal don't seem to buy hot sauce.\n",
    "\n",
    "### The Inspiration for MBA in AMR\n",
    "\n",
    "I thought AMR would be a good use case for this method because words (or $n$-gram sequences) could potentially be an equivalent scenario to a grocery store. Instead of milk, we have 'antimicrobial resistance.' And instead of receipts, we have research papers where the words potentially appear. \n",
    "\n",
    "I also think Market Basket Analysis is, essentially, the easiest Machine Learning technique for us sutdents to get down. Our machine thinks everything is associated to start with. Then, as we increase required support and confidence, we learn which itemsets are (and are not) associated with eachother. We could do this pretty naively, but, utilizing support's downward-closure gives us a nice performance bump to handle the usual use-case.\n",
    "\n",
    "I may be slightly concerned that our use case may not be large enough. I think only 4-ish thousand papers had associated PDFs that I then cleaned up. We may want to go back at this with a web scraper to find the other 20k papers that were found when searching with `getpapers`. \n",
    "\n",
    "I think I read somewhere that I need at least a few hundred supporting observations before a rule sould be considered statistically significant. And several hundred is not too small compared to 4,000 (in the general field of data science, I think). I'll have to keep this in mind when working on this.\n",
    "\n",
    "## The Code \n",
    "\n",
    "To get association rules for our text data set, I'll use the [apyori](https://pypi.org/project/apyori/) package for it's Apriori implementation. (I believe a similar implementation also exists in the [mlxtend](https://pypi.org/project/mlxtend/) package.) \n",
    "\n",
    "This implentation is looking for a 'list of lists' to operate on, so I'll need to read in each clean text file. Then I'll create list containing an entry for each gram in the file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package wordnet to /home/tanner/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    }
   ],
   "source": [
    "# Imports -------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import searched_terms\n",
    "# I know there's a better way to do this by appending to the path\n",
    "# sys.path.append('../src/')\n",
    "# But I couln't figure it out\n",
    "searched_terms = []\n",
    "with open('src/json/searched_terms.json', 'r') as searched_terms_file:\n",
    "    # Load in the json and pull out the array from the loaded dictionary\n",
    "    searched_terms = json.load(searched_terms_file)[\"searched_terms\"]\n",
    "\n",
    "# Lemmatize the searched terms\n",
    "# This puts our searched terms through the same relevent cleaning as the cleaned texts\n",
    "# Maximizing potential findings\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i, term in enumerate(searched_terms):\n",
    "    # Split the given term on space then lemmatize the word and rejoin with a space\n",
    "    cleaned_words = ' '.join([lemmatizer.lemmatize(word) for word in term.lower().split(' ')])\n",
    "    searched_terms[i] = cleaned_words\n",
    "\n",
    "# Set my working directory\n",
    "working_directory = Path('AmsContainer')\n",
    "\n",
    "# Glob (i.e. search) that driectory for cleaned files\n",
    "cleaned_text_paths = list(working_directory.glob('**/cleaned_output.txt'))\n",
    "\n",
    "word_transactions = []\n",
    "\n",
    "for cleaned_text_path in cleaned_text_paths:\n",
    "    with open(cleaned_text_path, 'r') as cleaned_text_file:\n",
    "        # Read the file and create bigrams\n",
    "        text = cleaned_text_file.read()\n",
    "        bigrams = ngrams(word_tokenize(text), 2)\n",
    "        \n",
    "\n",
    "        # The bigrams produced are intitially in the format:\n",
    "        # ('word_1', 'word_2'), but I'd like to work with:\n",
    "        # 'word_1 word_2' instead\n",
    "        # 'word_1 word_2' matches the format of searched_terms\n",
    "        bigrams = (' '.join(gram) for gram in bigrams)\n",
    "        \n",
    "        # This is too loose, need to figure out a better way to count\n",
    "        local_counts = []\n",
    "        #for term in searched_terms:\n",
    "        #    if term in text:\n",
    "        #        local_counts.append(1)\n",
    "        #    else:\n",
    "        #        local_counts.append(NAN)\n",
    "        for term in searched_terms:\n",
    "            if term in text:\n",
    "                local_counts.append(term)\n",
    "        word_transactions.append(local_counts)\n",
    "        #word_counts.append([1 if term in gram else 0 for term in searched_terms])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now that we have our 'list of lists,' we'll run this transaction list through `apyori`'s apriori implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "\n",
    "\n",
    "# I did say this was 'fast' and that 'we may not have enough data.' \n",
    "# I still stand by that, but this does seem to take a long time.\n",
    "# I think the word count is still pretty high even after cleaning.\n",
    "rules = apriori(word_transactions, \n",
    "                     min_support = 0.05, \n",
    "                     min_confidence = 0.80, \n",
    "                     min_lift = 1, \n",
    "                     max_length = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[None]\n"
    }
   ],
   "source": [
    "amr_rules = (rule if 'amr' in rule.items else None for rule in rules)\n",
    "one_health_rules = (rule if 'one health' else None in rule.items for rule in rules)\n",
    "union = (rule if rule in amr_rules and rule in one_health_rules else None for rule in rules)\n",
    "print(list(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizations\n",
    "\n",
    "Because every good notebook needs a few graphs.\n",
    "\n",
    "These associations (when results come back) would be really well represented by a network chart. I want to try creating one of these using [plot.ly](https://plot.ly/python/network-graphs/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "\n",
    "graph = nx.Graph() # Create a Graph object\n",
    "graph.add_nodes_from(searched_terms) # Use the searched terms as nodes\n",
    "\n",
    "edges = []\n",
    "# Create Edges\n",
    "for rule in rules:\n",
    "    for term in searched_terms:\n",
    "        if term in rule.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}