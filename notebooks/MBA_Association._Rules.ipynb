{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Basket Text Analysis of One Health and Antimicrobial Research Papers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### What is Market Basket Analysis?\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "Too long; Didn't Read: We're looking for words that appear commonly together.\n",
    "\n",
    "Consider a grocery store. Over the course of a month, **1000s** of people while have stumbled through our example grocery store. Some customers, like my parents, may pile up shopping carts to the cieling with microwave dinners. Customers a little more like me, or the average college student, might walk out with fewer than a dozen items. \n",
    "\n",
    "Consider how many different items exist in any given grocery store. Ghetto Kroger probably has several hundred items while Gucci Kroger has maybe more than a thousand? \n",
    "\n",
    "Out of all the possible things we could buy in a grocery store, customers end up only buying a select few. On top of that, we could reasonably guess that most purchases are biased towards a few items. I buy a half-gallon of milk, a loaf of bread, an a box of Lucky Charms when I go to the grocery store.\n",
    "\n",
    "Finding the most popular item is trivial. We can just keep a running count for each unique item and use a bar chart for visualization. Finding the most popular combination of items out of **every possible combination** is not so trivial. \n",
    "\n",
    "Think about the dataframe that would hold all of this. We'd have a column for every item and a row for every receipt. This dataframe could easily get to hundres of thousands of observations for several thousand variables in any decent suburb. Additionally, this matrix also tends to be sparse, meaning that almost entries are '0' or empty.\n",
    "\n",
    "#### The Solution\n",
    "Enter Association Rules or Market Basket Analysis. This tool uses a few simple steps to find probabilites that: given 'item a' ('on the left', 'leftside') is purchased, 'item b' ('on the right', 'rightside') is also purchased.\n",
    "\n",
    "Mathematically, let $X$, $Y$ be itemsets and $T$ be the set of all transactions taken place. Then an association rule looks simply like, \n",
    "$$ X \\Rightarrow Y $$\n",
    "\n",
    "\n",
    "Three terms we'll need to define:\n",
    "\n",
    "Too long; Didn't Read: Use a couple rules to find probably associated items in a dataset.\n",
    "\n",
    "1. Support - How many times the leftside appears in the dataset\n",
    "$$ supp(X) =  \\frac{|t \\in T; X \\in t|}{|T|}$$\n",
    "\n",
    "2. Confidence - The proportion of all transactions that contain both the left and right side. Or, a little more correctly, given a transaction contains the left side ($X$) the transaction also contains the right side ($Y$)\n",
    "$$ conf (X \\Rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)} $$\n",
    "\n",
    "My Latex is not quite pro, yet. My $f$ in the above looks a little whack, and I can't figure it out.\n",
    "\n",
    "3. Lift - The ratio of a given rule compared to what would be expected if th left and right sides truly appeared independent of one another.\n",
    "$$ lift(X \\Rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X) \\times supp(Y)} $$\n",
    "\n",
    "These are 'measures of interestingness'. There are others, but, in my mind, these are 'the big three'.\n",
    "\n",
    "Finding Association Rules is a two step process:\n",
    "\n",
    "1. Set some minimum support threshold to find frequent itemsets in the transaction database. Itemsets with less than this support are thrown out.\n",
    "2. Set some minimum confidence threshold to these supported itemsets in order to form rules. Rules with less than this confidence are thrown out.\n",
    "\n",
    "Once we have a few itemsets, the second step seems trivial to apply. The first step, at a glance, may seem less trivial. The first step seems to involve searching through every possible combination of itemsets to meet our given support threshold. Well, in short, the support rule has a property ('downward-closure') which, for us, boils down to 'unsupported itemsets can't be contained in larger supported itemsets'. This property is what algorithms like [Apriori](http://rakesh.agrawal-family.com/papers/vldb94apriori.pdf) take advantage of to efficiently search these super large, sparse transaction databases.\n",
    "\n",
    "### The Inspiration for AMR\n",
    "\n",
    "I thought AMR would be a good use case for this method because words (or $n$-gram sequences) could potentially be an equivalent scenario to a grocery store. Instead of milk, we have 'antimicrobial resistance.' And instead of receipts, we have research papers where the words potentially appear. \n",
    "\n",
    "I also think Market Basket Analysis is, essentially, the easiest Machine Learning technique for us sutdents to get down. Our machine thinks everything is associated to start with. Then, as we increase required support and confidence, we learn which itemsets are (and are not) associated with eachother. We could do this pretty naively, but, utilizing support's downward-closure gives us a nice performance bump to handle the usual use-case.\n",
    "\n",
    "I may be slightly concerned that our use case may not be large enough. I think only 4-ish thousand papers had associated PDFs that I then cleaned up. We may want to go back at this with a web scraper to find the other 20k papers that were found when searching with `getpapers`. \n",
    "\n",
    "I think I read somewhere that I need at least a few hundred supporting observations before a rule sould be considered statistically significant. And several hundred is not too small compared to 4,000 (in the general field of data science, I think). I'll have to keep this in mind when working on this.\n",
    "\n",
    "\n",
    "## The Code \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}